# ğŸ“˜ SpaceNet7 Endâ€‘toâ€‘End Geospatial ML Pipeline  
*A productionâ€‘style geospatial ML system with ETL, feature engineering, temporal modeling, API deployment, testing, and Dockerization.*

---

## ğŸš€ Overview

This repository implements a **full machine learning engineering workflow** using the **SpaceNet7** multiâ€‘temporal satellite imagery dataset. It demonstrates realâ€‘world engineering practices across:

- Largeâ€‘scale geospatial ETL  
- Spatial processing and geometry reconstruction  
- Data warehousing with Postgres/PostGIS  
- Orchestration with Prefect  
- Temporal feature engineering  
- Leakageâ€‘aware ML modeling  
- Packaged inference pipeline  
- FastAPI deployment (single + batch prediction)  
- Dockerized serving  
- Full pytest suite  
- Clear, professional documentation  

The goal is to mirror how **commercial ML engineering teams** build systems: modular, testable, containerized, deployable, and fully reproducible.

---

# ğŸ›°ï¸ Geospatial ETL Pipeline (Weeks 1â€“2)

The ETL pipeline ingests **6.6M+ pixelâ€‘level building labels** from SpaceNet7 and reconstructs all spatial and temporal metadata from scratch.

### **Key capabilities**
- Parse filenameâ€‘encoded metadata (chip ID, AOI, year, month, UTM coordinates)
- Build chip geometries (bounding boxes + centroids)
- Generate AOI polygons from chip centroids
- Assign chips to AOIs via spatial join
- Construct a **star schema** in Postgres/PostGIS:
  - `dim_chip`
  - `dim_aoi`
  - `dim_time`
  - `fact_chip_observation`
- Orchestrate the workflow using Prefect

### **Dataset summary**
- **6,664,652** pixelâ€‘level building observations  
- **60** chips across **30** AOIs  
- Multiple time periods  
- Raw CSV + filenameâ€‘encoded metadata  

The ETL has been validated endâ€‘toâ€‘end and successfully loads all data into Postgres.

---

# ğŸ§  ML Pipeline (Weeks 3â€“4)

This project models **monthly building count changes** at the chip level.

Each row in the feature table represents:

- a single **chip** (spatial tile)  
- at a single **time_id** (month)  
- with its **building_count** and **prev_building_count**  

### ğŸ¯ Target variable



\[
\Delta_t = \text{building\_count}_t - \text{building\_count}_{t-1}
\]



This project models **monthly building count changes** at the chip level.

Each row in the feature table represents:

- a single **chip** (spatial tile)  
- at a single **time_id** (month)  
- with its **building_count** and **prev_building_count**  

### ğŸ¯ Target variable



\[
\Delta_t = \text{building\_count}_t - \text{building\_count}_{t-1}
\]



This is a **regression problem**.

---

## ğŸ“Š Feature Engineering

Defined in `src/ml_end_to_end_pipeline/models/features.py`.

Features include:

- `chip_id` â€” categorical identifier  
- `building_count` â€” current month  
- `prev_building_count` â€” previous month  
- `delta_count` â€” regression target  

To avoid leakage, we **drop**:

- `year`, `month`  
- `aoi_id`  
- geometries (`geometry`, `centroid`)  

---

## ğŸ•’ Temporal Splitting

Implemented in `src/ml_end_to_end_pipeline/models/split.py`.

- **Train:** earliest months  
- **Validation:** middle months  
- **Test:** most recent months  

This preserves causal structure and prevents futureâ€‘toâ€‘past leakage.

---

## ğŸ¤– Modeling Pipeline

Defined in `src/ml_end_to_end_pipeline/models/pipeline.py`.

- Numeric features scaled  
- Categorical features oneâ€‘hot encoded  
- Model: `RandomForestRegressor`  
- Hyperparameter tuning via `GridSearchCV`  
- Evaluation via MAE, RMSE, RÂ²  

The best model is saved to:

```
models/best_regression_model.joblib
```

---

# ğŸ§ª Testing (Week 5)

A full pytest suite validates:

- Pipeline construction  
- Model loading  
- Singleâ€‘record inference  
- Batch inference  
- Feature engineering  
- Metadata parsing  

Run tests:

```bash
pytest -q
```

All tests pass:

```
6 passed in X.XXs
```

---

# ğŸŒ FastAPI Inference Service (Week 5)

A productionâ€‘ready API supports **single** and **batch** prediction.

Start the API:

```bash
uvicorn ml_end_to_end_pipeline.api.app:app --reload
```

### Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/health` | Health check |
| `POST` | `/predict` | Single prediction |
| `POST` | `/predict/batch` | Batch prediction |

Swagger UI:

```
http://localhost:8000/docs
```

---

# ğŸ³ Docker Deployment (Week 5)

Build the image:

```bash
docker build -t building-growth-api .
```

Run the container:

```bash
docker run -p 8000:8000 building-growth-api
```

Test the API:

```
http://localhost:8000/health
http://localhost:8000/docs
```

---

# ğŸ“¦ Installation

Install the package in editable mode:

```bash
pip install -e .
```

Import it:

```python
from ml_end_to_end_pipeline.models.predict import run_single_inference
```

---

# ğŸ”® Inference (CLI)

### Singleâ€‘record

```bash
python -m ml_end_to_end_pipeline.models.predict \
    --chip_id A123 \
    --building_count 50 \
    --prev_building_count 45
```

### Batch (CSV)

```bash
python -m ml_end_to_end_pipeline.models.predict \
    --input data/new_data.csv \
    --output predictions.csv
```

---

# ğŸ—ï¸ Architecture Diagram

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Raw Pixel CSV (6.6M)   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                        ETL Pipeline
      (parse â†’ geometry â†’ AOI â†’ star schema â†’ PostGIS)
                              â”‚
                              â–¼
                    Feature Engineering
                              â”‚
                              â–¼
                     Temporal Train/Val/Test
                              â”‚
                              â–¼
                    Regression Model (RF)
                              â”‚
                              â–¼
                   Saved Model Artifact
                              â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼               â–¼               â–¼
   predict.py       FastAPI Service     Tests
          â”‚               â”‚               â”‚
          â–¼               â–¼               â–¼
   CLI Inference     JSON API        CI-ready suite
          â”‚               â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Docker â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Week 6: API Testing, Logging, and Container Reliability

Weekâ€¯6 focused on validating the endâ€‘toâ€‘end inference API, improving observability, and ensuring the service runs reliably inside Docker. This was the first week where the full ML pipeline, model artifact, and FastAPI service were exercised together in a productionâ€‘like environment.

### Key Achievements

#### 1. API Testing & Validation
- Successfully tested `/health`, `/predict`, and `/predict/batch` endpoints using `curl`.
- Verified correct request/response schemas and JSON parsing.
- Confirmed that the regression model loads correctly inside the container and produces valid predictions.

#### 2. Logging Implementation
- Added structured logging to the FastAPI application, including:
  - Requestâ€‘level logs  
  - Latency measurement  
  - Model version tagging  
- Integrated a `logging_config.yaml` file and updated Uvicorn to use it.
- Installed `PyYAML` and validated that logs stream correctly via `docker logs -f`.

#### 3. Import & Inference Stability Fixes
- Resolved import errors by aligning API imports with the existing project structure (`models.predict`).
- Updated the API to load the model once at startup for efficient inference.
- Ensured DataFrame construction matches the modelâ€™s expected feature schema.

#### 4. Docker Reliability Improvements
- Rebuilt the Docker image with a clean build context and validated the internal file structure.
- Ensured all dependencies (including scikitâ€‘learn and PyYAML) are installed correctly.
- Verified that the API runs cleanly in detached mode and is reachable at `localhost:8000`.

### Outcome
By the end of Weekâ€¯6, the project had a fully functional, containerized inference API with reliable logging, stable imports, and validated prediction behavior. This foundation enables Weekâ€¯7â€™s focus on testing, linting, and CI/CD.


---


---

# ğŸŒ FastAPI Inference Service (Week 5)

A productionâ€‘ready API supports **single** and **batch** prediction.

Start the API:

```bash
uvicorn ml_end_to_end_pipeline.api.app:app --reload
```

### Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/health` | Health check |
| `POST` | `/predict` | Single prediction |
| `POST` | `/predict/batch` | Batch prediction |

Swagger UI:

```
http://localhost:8000/docs
```

---

# ğŸ³ Docker Deployment (Week 5)

Build the image:

```bash
docker build -t building-growth-api .
```

Run the container:

```bash
docker run -p 8000:8000 building-growth-api
```

Test the API:

```
http://localhost:8000/health
http://localhost:8000/docs
```

---

# ğŸ“¦ Installation

Install the package in editable mode:

```bash
pip install -e .
```

Import it:

```python
from ml_end_to_end_pipeline.models.predict import run_single_inference
```

---

# ğŸ”® Inference (CLI)

### Singleâ€‘record

```bash
python -m ml_end_to_end_pipeline.models.predict \
    --chip_id A123 \
    --building_count 50 \
    --prev_building_count 45
```

### Batch (CSV)

```bash
python -m ml_end_to_end_pipeline.models.predict \
    --input data/new_data.csv \
    --output predictions.csv
```

---

# ğŸ—ï¸ Architecture Diagram

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Raw Pixel CSV (6.6M)   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                        ETL Pipeline
      (parse â†’ geometry â†’ AOI â†’ star schema â†’ PostGIS)
                              â”‚
                              â–¼
                    Feature Engineering
                              â”‚
                              â–¼
                     Temporal Train/Val/Test
                              â”‚
                              â–¼
                    Regression Model (RF)
                              â”‚
                              â–¼
                   Saved Model Artifact
                              â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼               â–¼               â–¼
   predict.py       FastAPI Service     Tests
          â”‚               â”‚               â”‚
          â–¼               â–¼               â–¼
   CLI Inference     JSON API        CI-ready suite
          â”‚               â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Docker â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ—„ï¸ Database Schema

### `dim_chip`
Chipâ€‘level metadata including geometry and centroid.

### `dim_aoi`
AOI polygons reconstructed from chip centroids.

### `dim_time`
Year/month combinations extracted from filenames.

### `fact_chip_observation`
Buildingâ€‘level observations linked to chip, AOI, and time.

---

# ğŸ“š Tech Stack

### **Languages & Libraries**
- Python  
- scikitâ€‘learn  
- pandas  
- FastAPI  

### **Infrastructure & Tooling**
- Docker  
- Prefect  
- PostgreSQL + PostGIS  
- GitHub Actions  
- Azure (App Service + Container Registry)  
- MLflow or DVC (future)  

---

# ğŸ“‚ Repository Structure

```
ml-end-to-end-pipeline/
â”‚
â”œâ”€â”€ deployments/
â”‚   â””â”€â”€ metadata_etl.yaml
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ metadata.py
â”‚   â”‚   â”œâ”€â”€ load.py
â”‚   â”‚   â”œâ”€â”€ transform.py
â”‚   â”‚   â”œâ”€â”€ ingest.py
â”‚   â”‚   â”œâ”€â”€ build_aoi_polygons.py
â”‚   â”‚   â”œâ”€â”€ schema.py
â”‚   â”‚   â””â”€â”€ run_metadata_local.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â””â”€â”€ metadata_flow.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ml_end_to_end_pipeline/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ models/
â”œâ”€â”€ data/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ tests/
â””â”€â”€ README.md
```

---

# ğŸ—ºï¸ Roadmap

- **Week 1â€“2:** ETL + SQL + Prefect  
- **Week 3â€“4:** ML Pipelines  
- **Week 5:** API + Docker + Testing  
- **Week 6:** Cloud Deployment  
- **Week 7â€“8:** CI/CD  
- **Week 9â€“10:** MLOps (MLflow, DVC)  
- **Week 11â€“12:** Monitoring + Retraining  

---

# ğŸ”­ Future Improvements

- Add monitoring dashboards  
- Add automated retraining pipeline  
- Add feature store integration  
- Add spatial ML models (CNNs, Uâ€‘Nets)  
- Integrate raster imagery  
- Deploy to Azure App Service or AKS  

---